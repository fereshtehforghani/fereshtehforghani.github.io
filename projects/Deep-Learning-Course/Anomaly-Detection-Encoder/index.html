<!DOCTYPE html>
<html>
<head>
  <link rel="icon" href="/assets/img/main_icon.ico" type="image/x-icon">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Amin Fadaeinejad | Anomaly Detection using Auto-encoder</title>
  <meta name="description" content="This is the personal website of Amin Fadaeinejad">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">


<style>
table, th, td {
  border: 1px solid black;
  align-items: center;
}
</style>

</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Amin</span> Fadaeinejad</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/cv/">
                    Curriculum Vitae
                    
                  </a>
              </li>
                          
              <li class="nav-item ">
                  <a class="nav-link" href="/projects/">
                    Projects
                    
                  </a>
              </li>

            
              <li class="nav-item ">
                  <a class="nav-link" href="/publications/">
                    Publications
                    
                  </a>
              </li>
            
          <li class="nav-item ">
                  <a class="nav-link" href="/talks/">
                    Talks
                    
                  </a>
              </li>
          
              <li class="nav-item ">
                  <a class="nav-link" href="/teaching/">
                    Teaching
                  </a>
              </li>
            
          
            
          
        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
  <h1>Anomaly Detection with Auto-Encoders</h1>
 

<p><br /></p>
<div style="align: left; text-align:center;">
    <img src="/assets/img/Cut_res.JPG" width="100%" height="auto"  id="figure8" />
    <!-- <div class="caption" >Figure 1: My result from reproducing the <a href="https://arxiv.org/pdf/1312.4659.pdf">DeepPose</a></div> -->
</div>

<p><br /></p>

<p> In data science, anomaly detection is when the system tries to detect a rare item, event or observation which deviates significantly from the original distribution and can not be defined as normal behaviour. Three different categories of anomaly detection exist:</p>

<ul>
  <li>Supervised anomaly detection</li>
  <li>Semi-supervised anomaly detection</li>
  <li>Unsupervised anomaly detection</li>
</ul>

<p> In this project, we will be using an unsupervised approach to detect the regions of the hazelnut images that do not match the distribution.</p>

<h2 id="overview">1. Background</h2>

<h4 id="overview">1.1  Anomaly Detection</h4>

<p>Anomaly detection, also known as out-layer analysis, is a step-in data mining that identifies data points, events, and observations that deviate from a data set’s normal behaviour. It is the process of finding patterns in data that do not conform to prior expected behaviour. Anomaly detection is being employed increasingly in the presence of big data that is captured by sensors, social media platforms, huge networks, etc. They also have many applications, including energy systems, medical devices, banking network intrusion detection, etc. Machine learning is progressively being used to automate anomaly detection.</p>


<h4 id="overview"> 1.2. Auto-Encoders</h4>

<p>Auto Encoder is a generative unsupervised deep learning
the algorithm used for reconstructing high-dimensional input
data using a neural network with a narrow bottleneck layer in
the middle that contains the latent representation of the input
data. Auto-Encoders have two main parts called, Encoder
and Decoder. </p>


<div style="align: left; text-align:center;">
    <img src="/assets/img/Auto-Encoder.png" width="80%" height="auto" />
    <div class="caption" id="figure1" >Figure 1</div>
</div>


<ul>
  <li>Encoder: Accepts high-dimensional input data and translates it to latent low-dimensional data. The input size of an Encoder network is larger than its output size.</li>
  <li>Decoder: The Decoder network receives the input from the Encoder coder’s output. The Decoder’s objective is to reconstruct the input data. The output size of a Decoder network is larger than its input size.</li>
</ul>

<h4 id="overview">1.3 Accuracy methods </h4>

<p>In order to be able to compare our results with those already published, we need to have a standard accuracy method. First off, we need to know the concept of True and Predicted conditions. Two types of correct predictions (True Positives and True Negatives), and there are two types of errors. Error Type I for any observation predicted positive when it is negative (False Positive, also called False Alert). Error Type II for any observation predicted negative when it is positive (False Negative).
 </p>


<div style="align: left; text-align:center;">
    <img src="/assets/img/True Positive.ppm" width="65%" height="auto"  id="figure2" />
    <div class="caption" >Figure 2</div>
</div>


<h2 id="overview">2.  Dataset</h2>

<p>For our project, we used the MVTec dataset  that belonged
to <a href="https://www.semanticscholar.org/paper/The-MVTec-Anomaly-Detection-Dataset%3A-A-Real-World-Bergmann-Batzner/48f9a48aa5b1230b05a443d2d531e6441a541686">Paul Bergmann, January
2021</a>,this data set contained
several different groups of images such as Bottles, Cables, Capsule, Carpet, Grid, Hazelnut, Leather, Metal Nut,
Pill, Screw, Tile, Toothbrush, Transistor, Wood and Zipper.
Due to the limitation, we were able only to train one of the data sets for our project. We used the Hazelnut dataset, which contained 390 defect-free images. Deep neural networks are known for their impressive accuracy when trained with a large dataset. Therefore we used <strong>data augmentation</strong> to increase the number of data samples 
we have. We used rotation, flipping and
adding noise to increase the data samples. In the
end, we had approximately 2000 images. There are 70 test
images which different types, such as crack, cut, hole and
print. The dataset also includes ground Truth to calculate
the accuracy of the testing.</p>

<h4>2.1 Train Images</h4>

For training, we have 390 fine images of hazelnuts. look at figure <a href="#figure3">3</a>.


<div style="align: left; text-align:center;">
    <img src="/assets/img/Good_Data.JPG" width="80%" height="auto" id="figure3"/>
    <div class="caption">Figure 3</div>
</div>


<!-- <p style="text-align:center;"><img src="/assets/img/Good_Data.JPG" alt="Train Images" width="400px" height="200px" id="good_image" /></p> -->

<h4>2.1 Test Images</h4>

There are 4 different testing sets, including crack, cut, hole, and print. Figure <a href="#figure4">4</a> shows some samples of these test images.

<div style="align: left; text-align:center;">
    <img src="/assets/img/Def_Data.JPG" width="80%" height="auto" id="figure4" />
    <div class="caption">Figure 4</div>
</div>

Additional to the defective images, we have the segmentation of the defective region of the images.

<h2 id="overview">3.  Network Architect</h2>

Our model had two main parts the Encoder and the Decoder. Each one had several convolutional layers followed by Batch-Normalization with 0.1 momentum and ϵ = 10−6
. For the activation function, we used the LeakyReLU with negative slope 0.01.



<h4 id="overview">3.1  Encoder Architect</h4>

<a href="#table1"> Table 1</a> shows each layer and its dimensions for the Encoder.

<table style="width:100%" id="table1">
  <tr>
    <td>LAYER TYPE</td>
    <td>IN</td>
    <td>OUT</td>
    <td>KERNEL</td>
    <td>STRIDE</td>
  </tr>
  <tr>
    <td>CONV2D</td>
    <td>3</td>
    <td>3</td>
    <td>4</td>
    <td>2</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>3</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
<tr>
    <td>CONV2D</td>
    <td>3</td>
    <td>32</td>
    <td>4</td>
    <td>2</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>32</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
<tr>
    <td>CONV2D</td>
    <td>32</td>
    <td>32</td>
    <td>4</td>
    <td>2</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>32</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
<tr>
    <td>CONV2D</td>
    <td>32</td>
    <td>32</td>
    <td>3</td>
    <td>1</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>32</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>CONV2D</td>
    <td>32</td>
    <td>64</td>
    <td>4</td>
    <td>2</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>64</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
<tr>
    <td>CONV2D</td>
    <td>64</td>
    <td>64</td>
    <td>3</td>
    <td>1</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>64</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
<tr>
    <td>CONV2D</td>
    <td>64</td>
    <td>128</td>
    <td>4</td>
    <td>2</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>128</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
<tr>
    <td>CONV2D</td>
    <td>128</td>
    <td>64</td>
    <td>3</td>
    <td>1</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>64</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>CONV2D</td>
    <td>64</td>
    <td>32</td>
    <td>3</td>
    <td>1</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>32</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
<tr>
    <td>CONV2D</td>
    <td>32</td>
    <td>500</td>
    <td>8</td>
    <td>1</td>
  </tr>
  <tr>
    <td>BATCHNORM2D</td>
    <td>500</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
</table>
<center > Table 1</center>








  <!--
This part is for the decoder
-->

<h4 id="overview">3.2  Decoder Architect</h4>

<a href="#table2"> Table 2</a> shows each layer and its dimensions for the Dencoder.

<table style="width:100%" id="table2">
  <tr>
    <td>LAYER TYPE</td>
    <td>IN</td>
    <td>OUT</td>
    <td>KERNEL</td>
    <td>STRIDE</td>
  </tr>
  <tr>
    <td>CONVTRANSPOSE2D</td>
    <td>500</td>
    <td>32</td>
    <td>8</td>
    <td>1</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>32</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
<tr>
    <td>CONVTRANSPOSE2D</td>
    <td>32</td>
    <td>64</td>
    <td>3</td>
    <td>1</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>64</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
<tr>
    <td>CONVTRANSPOSE2D</td>
    <td>64</td>
    <td>128</td>
    <td>3</td>
    <td>1</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>128</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
<tr>
    <td>CONVTRANSPOSE2D</td>
    <td>128</td>
    <td>64</td>
    <td>4</td>
    <td>2</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>64</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>



  <tr>
    <td>CONVTRANSPOSE2D</td>
    <td>64</td>
    <td>64</td>
    <td>3</td>
    <td>1</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>64</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
<tr>
    <td>CONVTRANSPOSE2D</td>
    <td>64</td>
    <td>32</td>
    <td>4</td>
    <td>2</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>32</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
<tr>
    <td>CONVTRANSPOSE2D</td>
    <td>32</td>
    <td>32</td>
    <td>3</td>
    <td>1</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>32</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
<tr>
    <td>CONVTRANSPOSE2D</td>
    <td>32</td>
    <td>3</td>
    <td>4</td>
    <td>2</td>
  </tr>
  <tr>
    <td>BATCHNORM2D, LEAKYRELU</td>
    <td>64</td>
    <td></td>
    <td></td>
    <td></td>
  </tr>


  <tr>
    <td>CONVTRANSPOSE2D</td>
    <td>3</td>
    <td>3</td>
    <td>4</td>
    <td>2</td>
  </tr>
</table>
<center > Table 2</center>



<h2 id="more-information">4. Experiments</h2>

<h4 id="more-information">4.1  Training</h4>

The model has been trained over the mentioned dataset for 1000 epochs. We used the ``L2`` loss for training our network using the ADAM optimizer. Figure <a href="figure5">5</a> shows the loss for each step (each epoch has approximately 63 steps).

<div style="align: left; text-align:center;">
    <img src="/assets/img/Training_loss.JPG" id="figure5" width= "70%"  height="auto" />
    <div class="caption">Figure 5</div>
</div>


According the figure <a href="#figure2">2</a> we have an understanding towards
True Positive, True Negative, False Positive and False Negative. Based on that, it is common to analyze four metrics
which are:
<center>
  <div class="cmath"> Precision` = (TP)/(TP+FP)` ` ` ` ` ` ` ` ` ` ` ` ` Recall` = (TP)/(TP+FN)`</div>
  <div class="cmath"> Accuracy` = (TP + TN)/(TP + FP + FN + TN)`</div>
</center>

But because Precision and Recall usually play against each other, we may rely on their harmonic mean, which is the F1-score. 


<center>
<div class="cmath"> `F_1 = (2)/(Recall^(-1) + Precision^(-1)) = (2 * Recall *Precision)/(Recall + Precision)`</div>
</center>

Figure <a href="#figure7">7</a> shows the F1 score during training over the Hole and Print validation set.

<div style="align: left; text-align:center;">
    <img src="/assets/img/2_1-F1.JPG" id="figure6" width= "100%"  height="auto" />
    <div class="caption">Figure 6</div>
</div>

Figure <a href="#figure6">6</a> shows the F1 score during training over the Crack and Cut validation set.
<div style="align: left; text-align:center;">
    <img src="/assets/img/2_2-F1.JPG" id="figure7" width= "100%"  height="auto" />
    <div class="caption">Figure 7</div>
</div>

<h4 id="more-information">4.2  Methodology</h4>

We are going to train our network to reconstruct the input image from the latent space. We do that by feeding many normal images to the network so the network will learn the distribution of standard and defect-free images. Since the latent space is smaller than the size of the image, the network learns the essential components of the image. After that, since it has learned the critical part, whenever it sees a new image, it will try to use the things it learned to construct the image. Since a defective image has regions that deviate from its distribution, it will have a high constructed error (difference between the input and output), and by that, we find the defective pixels.

<ul>
  <li>Training: Input normal data to learn latent representation (Encoder). Use the latent representation to reconstruct the first image (Decoder).</li>
  <li>Testing: An image with a defect will have a different output compared to the expected input. Hence errors will be high. Apply a threshold for the reconstruction error to detect the error. </li>
</ul>

<div style="align: left; text-align:center;">
    <img src="/assets/img/Algorithm.JPG" id="figure8" width= "60%"  height="auto" />
    <div class="caption">Figure 8</div>
</div>

Figure <a href="#figure8">8</a> demonstrates the algorithm that we used to detect the pixels in which they were carrying outliers.

<h4 id="more-information">4.3  Outputs</h4>

We used the algorithm to detect the defect in the testing images, and for every dataset, here is the result. From left to right Original Image, Output Image, Predicted Pixels, Ground Truth.


<ul>
  <li>Crack:</li>
  <div style="align: left; text-align:center;">
    <img src="/assets/img/Crack_res.JPG" id="figure9" width= "100%"  height="auto" />
    <div class="caption">Figure 9, &#964 = 0.2</div>
</div>
  <li>Cut:</li>
  <div style="align: left; text-align:center;">
    <img src="/assets/img/Cut_res.JPG" id="figure10" width= "100%"  height="auto" />
    <div class="caption">Figure 10, &#964 = 0.2</div>
</div>
  <li>Hole:</li>
  <div style="align: left; text-align:center;">
    <img src="/assets/img/Hole_res.JPG" id="figure11" width= "100%"  height="auto" />
    <div class="caption">Figure 11, &#964 = 0.25</div>
</div>
  <li>Print:</li>
  <div style="align: left; text-align:center;">
    <img src="/assets/img/Print_res.JPG" id="figure12" width= "100%"  height="auto" />
    <div class="caption">Figure 12, &#964 = 0.2</div>
</div>
</ul>

The result can change depending on the value of &#964.


<h2 id="more-information">5. Discussions</h2>

<h4 id="more-information">5.1  Effect of Threshold</h4>

The output depends on the parameter &#964. In figures <a href="#figure13">13</a> and <a href="#figure14">14</a>, the effect of the threshold can be seen. If the threshold value is too high (for example, &#964 = 0.7 in figure <a href="#figure14">14</a>), the system will miss some defective pixels. Therefore the F1 accuracy will decrease. If the threshold value is too low (for example, &#964 = 0.1 in figure <a href="#figure14">14</a>), the system will assume that the defect-free pixels are also defective. In this case, the F1 will also decrease since many pixels do not contain faulty pixels, but the system will predict it anyway.


<div style="align: left; text-align:center;">
    <img src="/assets/img/Tau1.JPG" id="figure13" width= "70%"  height="auto" />
    <div class="caption">Figure 13</div>
</div>

Figure <a href="#figure14">14</a> shows the effect of different &#964 .From left to right, &#964=0.1, &#964=0.2, &#964=0.7.

<div style="align: left; text-align:center;">
    <img src="/assets/img/Tau2.JPG" id="figure14" width= "90%"  height="auto" />
    <div class="caption">Figure 14</div>
</div>


<h4 id="more-information">5.2 Effect of Latent Space size</h4>

In our model and result, the dimension of the latent space was 500. However, this is a hyper-parameter that we could change. Changing this parameter will cause effects on the output. As we mentioned, Decoder's objective is to reconstruct the input data from the output of the Encoder (aks the latent vector). If the latent size space is too small, the model will face difficulties learning small details since it has less power to learn the latent space distribution. Therefore in some cases, it is likely that some pixels have defected while they are entirely defect-free. If the latent space is too large, the model will start to memorize the input image instead of learning the image features. The model is likely to construct an image that has the same defective parts. This might lead to a situation that the model is not able to detect the faulty pixels in an image. 



<p>In the last part, we are going to compare our results with other
methods and approaches that are out there.
</p>



<h2 id="more-information">6. Conclusions</h2>


<p>In the last part, we are going to compare our results with other
methods and approaches that are out there.
</p>

<table style="width:100%">
  <tr>
    <td>METHODS</td>
    <td>RECALL</td>
    <td>PRECISION</td>
    <td>F1</td>
  </tr>
  <tr>
    <td>OUR METHOD (PRINT)</td>
    <td>0.22</td>
    <td>0.82</td>
    <td>0.35</td>
  </tr>
  <tr>
    <td>OUR METHOD (CRACK)</td>
    <td>0.26</td>
    <td>0.57</td>
    <td>0.36</td>
  </tr>
  <tr>
    <td>OUR METHOD (CUT)</td>
    <td>0.69</td>
    <td>0.8</td>
    <td>0.57</td>
  </tr>
  <tr>
    <td>OUR METHOD (HOLE)</td>
    <td>0.55</td>
    <td>0.61</td>
    <td>0.75</td>
  </tr>
  <tr>
    <td>AE(SSIM)</td>
    <td>0.08</td>
    <td><strong>1</strong></td>
    <td>0.15</td>
  </tr>
  <tr>
    <td><strong>AE(L2)</strong></td>
    <td><strong>0.84</strong></td>
    <td>0.93</td>
    <td><strong>0.88</strong></td>
  </tr>
  <tr>
    <td>ANOGAN</td>
    <td>0.16</td>
    <td>0.83</td>
    <td>0.27</td>
  </tr>
  <tr>
    <td>CNN FEATURE DICTIONARY</td>
    <td>0.07</td>
    <td>0.9</td>
    <td>0.13</td>
  </tr>
  
</table>

<p><br /></p>

<p>We compared our approach with some of the models that
are already used and it seems <a href="https://www.semanticscholar.org/paper/The-MVTec-Anomaly-Detection-Dataset%3A-A-Real-World-Bergmann-Batzner/48f9a48aa5b1230b05a443d2d531e6441a541686">Paul Bergmann, January
2021</a>  has the best performance when they train their model
on an L2 loss function. But it seems that our approach is
good since we outperform other methods such as AnoGANs,
Auto Encoders using SSIM and CNN feature Dictionary.
However, our approach has a lower accuracy in all 3 metrics
(Recall, Precision and F1) compared to the state-of-the-art
method. In one special case, the AE approach with the
SSIM loss function was able to reach 100 % for precision,
however, they were not able to have a good performance of
the Recall, therefor according to our F1 metric, our approach
outperforms the AE(SSIM) approach.
</p>

<h2 id="more-information">More infomation</h2>

<ul>
  <li>Code: <a href="https://github.com/aminfadaei116/Anomaly-Detection">link</a> </li>
  <li>Report: <a href="/assets/pdf/Projects-Reports/Anomaly-Report.pdf">link</a> </li>
</ul>

  </div>



  <!-- Footer -->
  <footer>
    &copy; Copyright 2023 Amin Fadaeinejad
    
    
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
