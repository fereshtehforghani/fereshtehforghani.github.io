<!DOCTYPE html>
<html>
<head>
  <link rel="icon" href="/assets/img/main_icon.ico" type="image/x-icon">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Amin Fadaeinejad | Unpaired Image-to-Image Translation</title>
  <meta name="description" content="This is the personal website of Amin Fadaeinejad">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <!-- <link rel="canonical" href="/projects/2_livex/"> -->


<style>
table, th, td {
  border: 1px solid black;
  align-items: center;
}
</style>

</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Amin</span> Fadaeinejad</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/cv/">
                    Curriculum Vitae
                    
                  </a>
              </li>
                          
              <li class="nav-item ">
                  <a class="nav-link" href="/projects/">
                    Projects
                    
                  </a>
              </li>

            
              <li class="nav-item ">
                  <a class="nav-link" href="/publications/">
                    Publications
                    
                  </a>
              </li>
            
          
          <li class="nav-item ">
                  <a class="nav-link" href="/talks/">
                    Talks
                    
                  </a>
              </li>
              
              <li class="nav-item ">
                  <a class="nav-link" href="/teaching/">
                    Teaching
                  </a>
              </li>
            
          
            
          
        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
  <h1>Image-to-Image Translation (CycleGAN)</h1>

<p><br /></p>

  <h4>Disclaimer</h4>
  I tried to reproduce the results from Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks <a href="https://junyanz.github.io/CycleGAN/">(project link)</a>. I had no contribution over the original paper, all I did was to recreate the same results from the paper.

<p><br /></p>
  <h4>Abstract</h4>
<p>
  
Image-to-image translation is one of the tasks in deep learning that has gotten a lot of attention in the deep learning and computer vision community. However, in the more traditional approaches, there must be labelled images for corresponding images, e.g. for every image from domain  <span class="emphasized">A</span> there must be an image  <span class="emphasized">B</span> in the other domain. However, in the approach that <a href="https://arxiv.org/pdf/1703.10593.pdf">Jun-Yan Zhu et el</a>, they did not need the corresponding images from the two domains.

</p>


<h2 id="overview">1. Introduction</h2>

<h4 id="overview">1.1  Paired Data</h4>

<p>In image-to-image translation tasks, one important thing is the need for paired data. <a href="#figure1">Figure 1</a> shows an example of paired data. However, there are limited datasets that have paired data. But CycleGAN introduces a new method to the image-to-image translation task without paired data. This is a significant boost in the field of image-to-image translation.</p>


<div style="align: left; text-align:center;">
    <img src="/assets/img/Pair2.PNG" width="65%" height="auto"  id="figure1" />
    <div class="caption" >Figure 1</div>
</div>

<h4 id="overview">1.2  Adversarial training</h4>

<p>
  <a href="#figure2">Figure 2(a)</a> shows the entire model, where we indicate that domain `X` is the real image and domain `Y` is the segmentation domain. Model `G` is a generator that converts images from domain `X` to domain `Y`, in other words, `G:X\rightarrow Y`. Model `F` is a generator that converts images from domain `Y` to domain `X`, in other words, `F:Y\rightarrow X`. There are also two discriminators whose objective is to detect whether the input image is real or fake. Discriminators `D_{X}` will detect whether the ground image (the real image from the scene) is real. Discriminators `D_{Y}` will detect whether the segmentation is real.
</p>

<div style="align: left; text-align:center;">
    <img src="/assets/img/Cycle.JPG" width="85%" height="auto"  id="figure2" />
    <div class="caption" >Figure 2</div>
</div>



<h4 id="overview">1.3 Cycle Consistency</h4>

<p>
  Adversarial training can, in theory, learn mappings `G` and `F` that produce outputs identically distributed as target domains `Y` and `X,` respectively. However, with a large enough capacity, a network can map the same input images to any random permutation of images in the target domain, where any of the learned mappings can induce an output distribution that matches the target distribution. Thus, adversarial losses alone cannot guarantee that the learned function can map an individual input `x_i` to a desired output `y_i.` The learned mapping functions should be cycle-consistent to reduce the space of possible mapping functions. <a href="#figure2">Figure 2(a)</a> shows the concept of cycle consistency. In this (cycle consistency) approach, after applying generator `G` to an image, we will also apply the function `F`, which will convert `G(X)` back to domain `Y`. The overall goal is to make `X` and `F(G(X))` as close as possible. We also do the same thing reversely, where the goal is to make `Y` and `G(F(Y))` as close as possible.
</p>

<p><br /></p>

<h4 id="overview">1.4 Identity loss</h4>

<p>
Besides the two methods we just mentioned, they also used one additional loss to train their model. If we apply generator `G` to an image that is in domain `X,` we expect it to stay in the same domain, which means `G(X)=X.` The opposite should also work, which means if we apply generator `F` to an image from domain `Y,` it should remain in domain `Y.` <a href="#figure3">Figure 3</a> shows this concept.
</p>

<div style="align: left; text-align:center;">
    <img src="/assets/img/P6.PNG" width="65%" height="auto"  id="figure3" />
    <img src="/assets/img/P7.PNG" width="65%" height="auto"  id="figure3" />
    <div class="caption" >Figure 3</div>
</div>

<h2 id="overview">2.  Dataset</h2>

<p>We used the <a href="https://www.cityscapes-dataset.com/">Cityscape</a> dataset, which contains images and their corresponding segmentation. <a href="#figure1">Figure 1</a> shows a sample of these images.</p>

<h2 id="overview">3.  Network Architecture</h2>

<p>
  We used <a href="https://arxiv.org/abs/1603.08155">Johnson et al</a> model since Johnson et al were able to achieve impressive results for neural style transfer and super-resolution. The network contains three convolutions, several <a href="https://arxiv.org/abs/1512.03385">residual blocks</a>, two fractionally-strided convolutions with stride 0.5, and one convolution that maps features to RGB. They use 6 blocks for 128 × 128 images and 9 blocks for 256×256 and higher-resolution training images. They also used <a href="https://arxiv.org/abs/1607.08022">instance normalization</a> in their model. For the discriminator networks, they used 70 × 70
<a href="https://arxiv.org/abs/1611.07004v3">PatchGAN</a> which aims to classify whether
70 × 70 overlapping image patches are real or fake.
</p>

<h2 id="overview">4.  Training</h2>

<p>
  We trained the model using a pre-trained model(trained on a different dataset), with a learning rate of 0.0002. We divided the objective (loss) by 2 while optimizing `D`, which slows down the rate at which `D` learns
relative to the rate of `G`. They kept the same learning rate
for the first 100 epochs and linearly decayed the rate to zero
over the following 100 epochs. Weights are initialized from a
Gaussian distribution `N(0, 0.02)`.

</p>

<h4>4.1 Discriminator loss</h4>

<div style="align: left; text-align:center;">
    <img src="/assets/img/DiscLoss.PNG" width="80%" height="auto"  id="figure4" />
    <div class="caption" >Figure 4</div>
</div>

<h4>4.2 Cycle Consistency loss</h4>

<div style="align: left; text-align:center;">
    <img src="/assets/img/CycleLoss.PNG" width="80%" height="auto"  id="figure5" />
    <div class="caption" >Figure 5</div>
</div>

<h4>4.3 Generator loss</h4>

<div style="align: left; text-align:center;">
    <img src="/assets/img/GenLoss.PNG" width="80%" height="auto"  id="figure6" />
    <div class="caption" >Figure 6</div>
</div>

<h4>4.4 Discriminator Accuracy</h4>

We trained our model for 200 epochs. <a href="#figure7">Figure 7</a> shows the accuracy of the discriminator, where it shows the percentage of correct guesses for the discriminator for detecting real ground images or fake ground images.


<div style="align: left; text-align:center;">
    <img src="/assets/img/acc.jpg" width="80%" height="auto"  id="figure7" />
    <div class="caption" >Figure 7</div>
</div>






<h2 id="more-information">5. Results</h2>

We conducted two different experiments 1. We only used the cycle consistency loss 2. In addition to the cycle consistency loss, we used identity loss. In the second case, we assumed that `
\lambda_{C} = \lambda_{I}`. Where `\lambda_{C}` is the coefficient of cycle consistency loss, and `\lambda_{I}` is the coefficient of identity loss.

<p><br /></p>

<h4>5.1 Cycle Consistency Loss Only</h4>

<p>
  We trained our model for 200 epochs. <a href="#figure7">Figure 7</a>(left) shows the accuracy of the discriminator, where it shows the percentage of correct guesses for the discriminator for detecting real ground images of fake ground images.
Our results and output were quite reasonable; however, we could not train the entire model to its full potential due to our limited resources. The results were not bad. The performance for generating ground images was better. <a href="#figure8">Figure 8</a> & <a href="#figure9">Figure 9</a> shows the result of our CycleGAN after 200 epochs of training without applying the identity loss.

</p>



<div style="align: left; text-align:center;">
    <img src="/assets/img/GroundGood1.PNG" width="60%" height="auto"  id="figure8" />
    <div class="caption" >Figure 8</div>
</div>

<div style="align: left; text-align:center;">
    <img src="/assets/img/Seg2.PNG" width="60%" height="auto"  id="figure9" />
    <div class="caption" >Figure 9</div>
</div>

<h4>5.2 Cycle Consistency Loss + Identity Loss</h4>

<p>
  Since we had limited access to Google Colab servers, we used the weights from the previous model instead of just re-training our model (the model with no Identity loss). We fine-tuned the model with the new hyper-parameters. The only difference in the two hyper-parameters is that in the case we used the identity loss, parameter `\lambda_{I}` was not 0 and was the same value as `\lambda_(C)`. We trained our model for another 70 epochs, and <a href="#figure7">figure 7</a>(right) shows the model's accuracy during training. This accuracy indicates the number of times the model has detected if the image was real or fake. Since we were fine-tuning the model, the first 200 epochs are the same as in figure <a href="#figure7">figure 7</a>(right). The final result from fine-tuning the model is in <a href="#figure10">figure 10</a> and <a href="#figure10">figure 11</a>. We can see the significant improvement in the ground image result in figure <a href="#figure10">figure 10</a>. Meanwhile, the result for the segmentation model in <a href="#figure11">figure 11</a> did not show much improvement.

</p>


<div style="align: left; text-align:center;">
    <img src="/assets/img/GroundGood.PNG" width="60%" height="auto"  id="figure10" />
    <div class="caption" >Figure 10</div>
</div>

<div style="align: left; text-align:center;">
    <img src="/assets/img/Seg1.PNG" width="60%" height="auto"  id="figure11" />
    <div class="caption" >Figure 11</div>
</div>

<h2 id="more-information">6. Conclusion</h2>

<p>
  To sum up, the result we got from this project was not the exact same as the main paper itself. However, the results were quite acceptable. By looking at the generated images, we could see that the model's performance over generated ground images is quite well, but of course not as good as the main images. The results of the generated segmentation images were not as good as the ground images. One thing that we need to consider is that every colour in the segmentation image is going to represent a particular class. However, what the CycleGAN model sees is a normal image, and it will treat the image the same way, which is why the segmentation results are not categorized. Since the segmentation output is not categorized(the output was just an RGB image), we could not fully reconstruct that part. It is also interesting since in the paper in (look at figure <a href="#figure12">figure 12</a>) they showed the result they had when they used Cycle Alone is quite similar to the result that we have gotten in our implementation. However, their final result is better than ours.
</p>

<div style="align: left; text-align:center;">
    <img src="/assets/img/tosh.PNG" width="80%" height="auto"  id="figure12" />
    <div class="caption" >Figure 12</div>
</div>



<a href="figure13">Figure 13</a> shows a sample result of the generated images for both real and segmentation images. Also you can check my Youtube video <a href="https://youtu.be/wXQPglO9ZTo">link</a> about this project.
<div style="align: left; text-align:center;">
    <img src="/assets/img/Final.PNG" width="80%" height="auto"  id="figure13" />
    <div class="caption" >Figure 13</div>
</div>


<h2 id="more-information">More infomation</h2>

<ul>
  <li>Code: <a href="https://github.com/aminfadaei116/CycleGan">link</a> </li>
  <li>Report: <a href="/assets/pdf/Projects-Reports/CycleGAN-Report.pdf">link</a> </li>
</ul>

  </div>



  <!-- Footer -->
  <footer>
    &copy; Copyright 2023 Amin Fadaeinejad
    
    
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
